# PIPELINE DEFINITION
# Name: simple-preprocessing-pipeline
# Description: Lee un CSV y hace preprocesamiento con pandas
components:
  comp-preprocess-data:
    executorLabel: exec-preprocess-data
    outputDefinitions:
      artifacts:
        processed_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-preprocess-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas==2.3.1'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_data(processed_data: Output[Dataset]) -> None:\n \
          \   \"\"\"\n    Preprocesses CSV data and outputs as Kubeflow artifact\n\
          \n    Args:\n        processed_data: Output dataset artifact containing\
          \ the processed CSV\n    \"\"\"\n    import pandas as pd\n\n    print(\"\
          \U0001F504 Starting data preprocessing...\")\n\n    # Load and process the\
          \ data (same logic as preprocess.py)\n    df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n\
          \    print(f\"\U0001F4CA Loaded dataset with {len(df)} rows and {len(df.columns)}\
          \ columns\")\n\n    # Add feature engineering\n    df['sepal_area'] = df['sepal_length']\
          \ * df['sepal_width']\n    print(\"\u2705 Added sepal_area feature\")\n\n\
          \    # Save directly to the Kubeflow artifact path\n    df.to_csv(processed_data.path,\
          \ index=False)\n    print(f\"\U0001F4BE Processed data saved to artifact:\
          \ {processed_data.path}\")\n\n    # Log summary statistics\n    print(f\"\
          \U0001F4C4 Final dataset: {len(df)} rows, {len(df.columns)} columns\")\n\
          \    print(f\"\U0001F4C8 Columns: {list(df.columns)}\")\n\n    return None\n\
          \n"
        image: python:3.9
pipelineInfo:
  description: Lee un CSV y hace preprocesamiento con pandas
  name: simple-preprocessing-pipeline
root:
  dag:
    tasks:
      preprocess-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-data
        taskInfo:
          name: Preprocess Data
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.1
